{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44092e2d-c0ee-46d7-9557-b11083feea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quest-1\n",
    "Ridge regression helps to prevent overfitting and improve the models performance, \n",
    "especially when dealing with correlated predictors\n",
    "OLS is a traditional method for linear regression that finds the best-fitting line through the data by minimizing the sum of the squared errors between the predicted and actual values\n",
    "\n",
    "#quest-2\n",
    "Linearity: There is a linear relationship between the dependent variable and the independent variables.   \n",
    "Independence: The observations are independent of each other.   \n",
    "Homoscedasticity: The variance of the residuals is constant across all values of the independent variables\n",
    "\n",
    "#quest-3\n",
    "The tuning parameter, lambda (λ), in Ridge Regression controls the amount of shrinkage applied to the coefficients. Choosing the optimal lambda is crucial for model performance.\n",
    "Cross-validation:This is the most common and reliable method.\n",
    "The dataset is divided into k folds. The model is trained on k-1 folds and tested on the remaining fold. This process is repeated k times, with each fold serving as the test set once. \n",
    "\n",
    "#quest-4\n",
    "while Ridge regression is excellent for handling multicollinearity and improving model stability, its not the ideal choice for feature selection. \n",
    "Lasso regression is generally preferred for this purpose\n",
    "\n",
    "#quest-5\n",
    "Multicollinearity: This occurs when independent variables are highly correlated with each other. In such cases, OLS regression coefficients become unstable and have large standard errors.   \n",
    "Ridge regression solution: It introduces a penalty term to the loss function, which shrinks the coefficients towards zero. This shrinkage helps to stabilize the coefficients and reduce their variance\n",
    "\n",
    "#quest-6\n",
    "Yes, Ridge Regression can handle both categorical and continuous independent variables.To incorporate categorical variables into a Ridge Regression model, you typically need to convert them into numerical representations. This is commonly done through:\n",
    "One-Hot Encoding: For nominal categorical variables (no inherent order), create dummy variables for each category.\n",
    "Ordinal Encoding: For ordinal categorical variables (with an inherent order), assign numerical values to categories based\n",
    "on their order.\n",
    "\n",
    "#quest-7\n",
    "A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. \n",
    "A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease.\n",
    "\n",
    "#quest-8\n",
    "Ridge Regression can be a useful tool for time series forecasting, especially when dealing with multicollinearity or when the underlying relationship between the target variable and its predictors is linear. \n",
    "However, its essential to consider the specific characteristics of the time series data and explore other time series modeling techniques as well.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
