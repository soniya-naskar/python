{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9004b3d7-3593-47b7-bcf1-4df950463bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Elastic net linear regression uses the penalties from both the lasso and ridge techniques to regularize regression models. The technique combines both the lasso and ridge regression methods by learning from their shortcomings to improve the regularization of statistical models.\n",
    "\n",
    "2.In practice, the optimal value of λ and the type of regularization (L1, L2, or Elastic Net) are often selected through cross-validation, where multiple models are trained with different values of λ and possibly different types of regularization\n",
    "\n",
    "3.Elastic net regression can select a subset of correlated features and avoid the instability of lasso regression. Advantages: - Combines Lasso and Ridge penalties, balancing between variable selection and coefficient shrinkage. - Can handle highly correlated data and perform variable selection\n",
    "\n",
    "4.Elastic net can also be used in other applications, such as in sparse PCA, where it obtains principal components that are modified by sparse loadings. The other application is in the kernel elastic net, where the generation of class kernel machines takes place with support vectors.\n",
    "\n",
    "5.The coefficients of elastic net regression represent the linear relationship between the features and the target variable, adjusted by the regularization terms. The larger the absolute value of a coefficient, the stronger the effect of the corresponding feature on the target variable.\n",
    "\n",
    "6.Mean/Median/Mode Imputation: Replace missing values with the mean (for numerical data), median (for numerical data), or mode (for categorical data) of the observed values.\n",
    "\n",
    "Example: For a numerical feature with missing values, you might replace missing values with the mean of that feature.\n",
    "\n",
    "7.Elastic Net Regression is a versatile tool that combines the penalties of both Lasso (L1) and Ridge (L2) regression.\n",
    "It is particularly useful for feature selection due to its ability to both shrink coefficients and select a subset of features.\n",
    "\n",
    "8.Using pickle , simply save your model on disc with dump() function and de-pickle it into your python code with load() function. Use open() function to create and/or read from a . pkl file and make sure you open the file in the binary format by wb for write and rb for read mode.\n",
    "\n",
    "9."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
