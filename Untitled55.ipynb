{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12dac8-c212-40c2-a728-55b851abe5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.You can use linear regression when you want to predict a continuous dependent variable from a scale of values. \n",
    "Use logistic regression when you expect a binary outcome (for example, yes or no). Here are examples of linear regression:\n",
    "Predicting the height of an adult based on the mothers and fathers height.\n",
    "\n",
    "2.The cost function for logistic regression is the average of the log loss over all training examples.\n",
    "It is often referred to as the cross-entropy cost function and is designed to optimize the parameters to minimize the prediction error for binary classification tasks\n",
    "\n",
    "3.Regularization helps you avoid overfitting by adding a penalty term to the cost function of your model, which measures how well your model fits the data. \n",
    "The penalty term reduces the complexity of your model by shrinking or eliminating some of the coefficients of your input variables\n",
    "\n",
    "4.The ROC curve is a graphical representation of the trade-off between true positive rate and false positive rate at various thresholds. It shows the performance of a classification model at different classification thresholds. \n",
    "The AUC (Area Under the Curve) is a summary measure of the ROC curve performance.\n",
    "\n",
    "5.Recursive Feature Elimination (RFE) is a feature selection technique that involves training a model on a subset of the features, and then iteratively removing the least important features one by one until we are left with the desired number of features.\n",
    "\n",
    "6.The best ways to handle class imbalance in logistic regression include oversampling minority classes, undersampling majority classes, using synthetic data generation techniques like SMOTE, or using class weights to penalize misclassifications\n",
    "\n",
    "7.Logistic regression faces challenges such as multicollinearity, overfitting, and assuming a linear relationship between predictors and outcome log-odds. These issues can lead to unstable coefficient estimates, overfitting, and difficulty generalizing the model to new data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
