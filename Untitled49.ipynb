{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c8f8f-8f59-43f9-9819-2e22aaece99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quest-1\n",
    "Lasso Regression, short for Least Absolute Shrinkage and Selection Operator, is a regularization technique used in linear regression to prevent overfitting and improve model prediction accuracy.\n",
    "Like Ridge Regression, Lasso also adds a penalty term to the loss function to prevent overfitting. However, Lasso uses L1 regularization (sum of absolute values of coefficients) while Ridge uses L2 regularization (sum of squared values of coefficients).\n",
    "\n",
    "#quest-2\n",
    "By introducing an L1 penalty, Lasso shrinks some coefficients to exactly zero, effectively removing irrelevant features from the model.\n",
    "This results in:   \n",
    "Model simplification: A more parsimonious model with fewer features.   \n",
    "Improved interpretability: Easier to understand the impact of important features on the target variable.   \n",
    "Reduced overfitting: By removing irrelevant features, the model is less likely to overfit the training data.  \n",
    "\n",
    "#quest-3\n",
    "Lasso regression and force coefficients toward 0. The smaller the coefficient the less important it is or less variance it explains. The actual value here will be less important since it will be used in logistic regression because it will end up being used in an exponential\n",
    "\n",
    "#quest-4\n",
    "Lasso Regression primarily has one tuning parameter:\n",
    "Lambda (λ): This parameter controls the strength of the L1 regularization.\n",
    "A larger λ value increases the amount of regularization, leading to more coefficients being pushed towards zero.\n",
    "\n",
    "#quest-5\n",
    "Regularization with a lasso penalty is an advantageous in that it reduces some unknown parameters in linear regression models toward exactly zero. We propose imposing a weighted lasso penalty on a nonlinear regression model and thereby selecting the number of basis functions effectively.\n",
    "\n",
    "#quest-6\n",
    "Lasso regression is preferred when the goal is feature selection, resulting in a simpler and more interpretable model with fewer variables. Ridge regression tends to favor a model with a higher number of parameters, as it shrinks less important coefficients but keeps them in the model.\n",
    "\n",
    "#quest-7\n",
    "Lasso regression can handle some multicollinearity without negatively impacting interpretability of the model, but it cannot overcome severe multicollinearity4. If covariates are highly correlated, lasso regression will arbitrarily drop one of the features from the model.\n",
    "\n",
    "#quest-8\n",
    "By cross valida\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
